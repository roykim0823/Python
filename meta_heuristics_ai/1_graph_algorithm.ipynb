{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.adjacency_list = []\n",
    "        self.visited = False\n",
    "\n",
    "\n",
    "def breadth_first_search(start_node):\n",
    "\n",
    "    # FIFO: first item we insert will be the first one to take out\n",
    "    queue = [start_node]\n",
    "\n",
    "    # we keep iterating (considering the neighbors) until the queue becomes empty\n",
    "    while queue:\n",
    "\n",
    "        # remove and return the first item we have inserted into the list\n",
    "        actual_node = queue.pop(0)\n",
    "        actual_node.visited = True\n",
    "        print(actual_node.name)\n",
    "\n",
    "        # let's consider the neighbors of the actual_node one by one\n",
    "        for n in actual_node.adjacency_list:\n",
    "            if not n.visited:\n",
    "                queue.append(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# we can create the nodes or vertices\n",
    "node1 = Node(\"A\")\n",
    "node2 = Node(\"B\")\n",
    "node3 = Node(\"C\")\n",
    "node4 = Node(\"D\")\n",
    "node5 = Node(\"E\")\n",
    "\n",
    "# we have to handle the neighbors\n",
    "node1.adjacency_list.append(node2)\n",
    "node1.adjacency_list.append(node3)\n",
    "node2.adjacency_list.append(node4)\n",
    "node4.adjacency_list.append(node5)\n",
    "\n",
    "# A->B -> D\n",
    "#  ->C -> E\n",
    "\n",
    "# run the BFS\n",
    "breadth_first_search(node1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WebCrawler with BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "class WebCrawler:\n",
    "\n",
    "    def __init__(self, max = 5):\n",
    "        # we want to avoid revisiting the same website over and over again\n",
    "        self.discovered_websites = []\n",
    "        self.max = max\n",
    "\n",
    "    # BFS implementation\n",
    "    def crawl(self, start_url):\n",
    "\n",
    "        queue = [start_url]\n",
    "        self.discovered_websites.append(start_url)\n",
    "\n",
    "        # THIS IS A STANDARD BREADTH-FIRST SEARCH\n",
    "        cnt = 0\n",
    "        while queue:\n",
    "            # for early stop\n",
    "            if cnt == self.max:\n",
    "                break\n",
    "            else:\n",
    "                cnt += 1\n",
    "\n",
    "            actual_url = queue.pop(0)\n",
    "            print(actual_url)\n",
    "            \n",
    "            \n",
    "\n",
    "            # this is the raw html representation of the given website (URL)\n",
    "            actual_url_html = self.read_raw_html(actual_url)\n",
    "\n",
    "            for url in self.get_links_from_html(actual_url_html):\n",
    "                if url not in self.discovered_websites:\n",
    "                    self.discovered_websites.append(url)\n",
    "                    queue.append(url)\n",
    "\n",
    "    def get_links_from_html(self, raw_html):\n",
    "        return re.findall(\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", raw_html)\n",
    "\n",
    "    def read_raw_html(self, url):\n",
    "\n",
    "        raw_html = ''\n",
    "\n",
    "        try:\n",
    "            raw_html = requests.get(url).text\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        return raw_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.cnn.com\n",
      "https://lightning.cnn.com\n",
      "https://aswpsdkus.com\n",
      "https://media.cnn.com\n",
      "https://client-api.arkoselabs.com\n",
      "https://middycdn-a.akamaihd.net\n"
     ]
    }
   ],
   "source": [
    "\n",
    "crawler = WebCrawler()\n",
    "crawler.crawl('https://www.cnn.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.adjacency_list = []\n",
    "        self.visited = False\n",
    "\n",
    "\n",
    "def depth_first_search(start_node):\n",
    "\n",
    "    # that we need a LIFO: last item we insert is the first one we take out\n",
    "    stack = [start_node]\n",
    "\n",
    "    # let's iterate until the stack becomes empty\n",
    "    while stack:\n",
    "\n",
    "        # the pop() function returns with the last item we have inserted - O(1)\n",
    "        actual_node = stack.pop()\n",
    "        actual_node.visited = True\n",
    "        print(actual_node.name)\n",
    "\n",
    "        for n in actual_node.adjacency_list:\n",
    "            # if the node has not been visited so far\n",
    "            if not n.visited:\n",
    "                # insert the item into the stack\n",
    "                stack.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "C\n",
      "B\n",
      "D\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# first we have to create the vertices (nodes)\n",
    "node1 = Node(\"A\")\n",
    "node2 = Node(\"B\")\n",
    "node3 = Node(\"C\")\n",
    "node4 = Node(\"D\")\n",
    "node5 = Node(\"E\")\n",
    "\n",
    "# handle and set the neighbors accordingly\n",
    "node1.adjacency_list.append(node2)\n",
    "node1.adjacency_list.append(node3)\n",
    "node2.adjacency_list.append(node4)\n",
    "node4.adjacency_list.append(node5)\n",
    "\n",
    "# run the DFS\n",
    "depth_first_search(node1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "D\n",
      "E\n",
      "C\n"
     ]
    }
   ],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.adjacency_list = []\n",
    "        self.visited = False\n",
    "\n",
    "# DFS with Recursion\n",
    "def depth_first_search(node):\n",
    "\n",
    "    node.visited = True\n",
    "    print(node.name)\n",
    "\n",
    "    for n in node.adjacency_list:\n",
    "        if not n.visited:\n",
    "            depth_first_search(n)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # first we have to create the vertices (nodes)\n",
    "    node1 = Node(\"A\")\n",
    "    node2 = Node(\"B\")\n",
    "    node3 = Node(\"C\")\n",
    "    node4 = Node(\"D\")\n",
    "    node5 = Node(\"E\")\n",
    "\n",
    "    # handle and set the neighbors accordingly\n",
    "    node1.adjacency_list.append(node2)\n",
    "    node1.adjacency_list.append(node3)\n",
    "    node2.adjacency_list.append(node4)\n",
    "    node4.adjacency_list.append(node5)\n",
    "\n",
    "    # run the DFS\n",
    "    depth_first_search(node1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFS vs DFS\n",
    "\n",
    "## BFS application\n",
    "- Pathfinding (BFS or DFS)\n",
    "- Maximum Flow, Edomnds-Karp algorithm uses BFS\n",
    "- Garbage Collection: Cheyen's algorithm uses BFS\n",
    "- Serialization (of tree like structure) since the order matters\n",
    "\n",
    "## DFS application\n",
    "- Pathfinding (BFS or DFS)\n",
    "- Topological Ordering (build tools)\n",
    "- Strongly connected components\n",
    "- Cycle Detection (in operation systems -> deadlock detection)\n",
    "\n",
    "## Memory Comparison (DFS preferred by less memory consumption)\n",
    "- BFS: in worse-case we have to store all the leaf nodes on the queue\n",
    "  - ex) balanced tree (N/2) -> O(n)\n",
    "  - it finds closer items faster.\n",
    "- DFS: in worst-case we have to stall all the nodes on the stack until we hit a leaf node\n",
    "  - ex) balanced tree -> the height of the tree -> O(longN)\n",
    "  - it finds the outlier items faster (when a verte is far away from the string node)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
